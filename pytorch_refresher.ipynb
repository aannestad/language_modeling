{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Thanks, Python engineer!\n",
    "\n",
    "################# 1,2 - In pytorch everything is a tensor ########################\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.empty(2,2,3)   # create empty tensors\n",
    "zeros = torch.zeros(2,3)\n",
    "ones = torch.ones(2,5, dtype=torch.int) # torch.float16\n",
    "#print(zeros)\n",
    "#print(ones.size())\n",
    "\n",
    "r2 = torch.rand(1,2)\n",
    "y = torch.tensor([2.4, 3])\n",
    "#print(torch.add(r2,y))\n",
    "#print((r2[0][0].item()))  # print value of tensor (type=float)\n",
    "\n",
    "#print(torch.rand(5,5).view(25))   # resize to 1-D tensor\n",
    "#print(torch.rand(4,4).view(-1,8))   # resize to 2x8 = 16 = 4x4\n",
    "\n",
    "a = torch.ones(10)\n",
    "#print(a)\n",
    "b = a.numpy()   # convert tensor to numpy.ndarray\n",
    "#print(type(b))\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)   # changes both, points to same place in memory\n",
    "\n",
    "b_t = torch.from_numpy(b)  # convert from numpy to tensor\n",
    "print(b_t)\n",
    "\n",
    "z = torch.ones(5, requires_grad=True)  # default is false, when calculates gradient\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1070, -1.2723,  0.2982], requires_grad=True)\n",
      "tensor([3.1070, 0.7277, 2.2982], grad_fn=<AddBackward0>)\n",
      "tensor([19.3073,  1.0592, 10.5633], grad_fn=<MulBackward0>)\n",
      "tensor(10.3099, grad_fn=<MeanBackward0>)\n",
      "tensor([4.1427, 0.9703, 3.0643])\n",
      "y=tensor([11.1070,  8.7277, 10.2982])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "# 3 - Fun with gradients using Autograd!\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x+2 \n",
    "\n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y*y*2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "z = z.mean() #  grad_fn=<MeanBackward0>\n",
    "#z = z\n",
    "print(z) # error:grad can be implicitly created only for scalar outputs\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "#z.backward(v) # dz/dx\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "# Disable gradient caculation, practical for inference:\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#y = x.detach()\n",
    "#with torch.no_grad():\n",
    "# x.requires_grad_(True)   # traling underscore -> modify the variable in-place\n",
    "#print(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 10\n",
    "    print(f\"y={y}\")   # no grad_fn=<AddBackward0>)\n",
    "\n",
    "# Calling backward function the gradient for the tensor will be accumulated into the .grad attribute\n",
    "# Example:\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*10).sum()\n",
    "    model_output.backward()                        # values are summed up (explodes) if no grad.zero()\n",
    "    print(f\"weights.grad={weights.grad}\")\n",
    "    weights.grad.zero_()              # gradients emptied, correct to do in training loop\n",
    "\n",
    "# Optimizer:\n",
    "\n",
    "#optimizer = torch.optim.SGD(weights, lr=0.1)\n",
    "#optimizer.step()\n",
    "#optimizer.zero_grad()\n",
    "\n",
    "# Remember to empty gradients:\n",
    "\n",
    "#z.backward()\n",
    "#weights.grad.zero_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Backpropagation\n",
    "\n",
    "# x -> a(x) -> y -> b(y) -> z\n",
    "# dz/dx = dz/dy * dy/dx  (chain rule to get the derivative of interest)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4cdbc7c3fa968f8f57a8da50a47c3ee9b8594a4de967130c021b1087ec3a6ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
