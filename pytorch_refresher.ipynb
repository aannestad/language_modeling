{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2., 2., 2., 2., 2., 2.])\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Thanks, Python engineer!\n",
    "\n",
    "################# 1,2 - In pytorch everything is a tensor ########################\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "x = torch.empty(2,2,3)   # create empty tensors\n",
    "zeros = torch.zeros(2,3)\n",
    "ones = torch.ones(2,5, dtype=torch.int) # torch.float16\n",
    "#print(zeros)\n",
    "#print(ones.size())\n",
    "\n",
    "r2 = torch.rand(1,2)\n",
    "y = torch.tensor([2.4, 3])\n",
    "#print(torch.add(r2,y))\n",
    "#print((r2[0][0].item()))  # print value of tensor (type=float)\n",
    "\n",
    "#print(torch.rand(5,5).view(25))   # resize to 1-D tensor\n",
    "#print(torch.rand(4,4).view(-1,8))   # resize to 2x8 = 16 = 4x4\n",
    "\n",
    "a = torch.ones(10)\n",
    "#print(a)\n",
    "b = a.numpy()   # convert tensor to numpy.ndarray\n",
    "#print(type(b))\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)   # changes both, points to same place in memory\n",
    "\n",
    "b_t = torch.from_numpy(b)  # convert from numpy to tensor\n",
    "print(b_t)\n",
    "\n",
    "z = torch.ones(5, requires_grad=True)  # default is false, when calculates gradient\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1070, -1.2723,  0.2982], requires_grad=True)\n",
      "tensor([3.1070, 0.7277, 2.2982], grad_fn=<AddBackward0>)\n",
      "tensor([19.3073,  1.0592, 10.5633], grad_fn=<MulBackward0>)\n",
      "tensor(10.3099, grad_fn=<MeanBackward0>)\n",
      "tensor([4.1427, 0.9703, 3.0643])\n",
      "y=tensor([11.1070,  8.7277, 10.2982])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n",
      "weights.grad=tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "# 3 - Fun with gradients using Autograd!\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y = x+2 \n",
    "\n",
    "print(y) # grad_fn=<AddBackward0>\n",
    "\n",
    "z = y*y*2\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "z = z.mean() #  grad_fn=<MeanBackward0>\n",
    "#z = z\n",
    "print(z) # error:grad can be implicitly created only for scalar outputs\n",
    "\n",
    "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
    "#z.backward(v) # dz/dx\n",
    "z.backward()\n",
    "print(x.grad)\n",
    "\n",
    "\n",
    "# Disable gradient caculation, practical for inference:\n",
    "\n",
    "#x.requires_grad_(False)\n",
    "#y = x.detach()\n",
    "#with torch.no_grad():\n",
    "# x.requires_grad_(True)   # traling underscore -> modify the variable in-place\n",
    "#print(x)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = x + 10\n",
    "    print(f\"y={y}\")   # no grad_fn=<AddBackward0>)\n",
    "\n",
    "# Calling backward function the gradient for the tensor will be accumulated into the .grad attribute\n",
    "# Example:\n",
    "\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*10).sum()\n",
    "    model_output.backward()                        # values are summed up (explodes) if no grad.zero()\n",
    "    print(f\"weights.grad={weights.grad}\")\n",
    "    weights.grad.zero_()              # gradients emptied, correct to do in training loop\n",
    "\n",
    "# Optimizer:\n",
    "\n",
    "#optimizer = torch.optim.SGD(weights, lr=0.1)\n",
    "#optimizer.step()\n",
    "#optimizer.zero_grad()\n",
    "\n",
    "# Remember to empty gradients:\n",
    "\n",
    "#z.backward()\n",
    "#weights.grad.zero_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss=1.0\n",
      "weight gradient=-2.0\n"
     ]
    }
   ],
   "source": [
    "# 4 - Backpropagation (THEORY)\n",
    "\n",
    "# x -> a(x) -> y -> b(y) -> z\n",
    "# dz/dx = dz/dy * dy/dx  (chain rule to get the derivative of interest)\n",
    "\n",
    "# Computational graph: x,y -> f(x,y) = x*y = z -> prediction\n",
    "# Partials of interest\n",
    "# dz/dy = d(x*y)/dy = x*(du/dy) = x*(y') = x*1 = x \n",
    "# dz/dx = d(x*y)/dx = y*(du/dx) = y*(x') = y*1 = y\n",
    "\n",
    "# Need this because of final loss function minimization wrt. inputs: dLoss/dx\n",
    "# Given dLoss/dz, we can easily get the gratient of loss wrt x by\n",
    "\n",
    "# dLoss/dx = dLoss/dz * dz/dx\n",
    "\n",
    "# STEPS in pipeline:\n",
    "\n",
    "# 1 - Forward pass: Compute loss\n",
    "# 2 - Compute local gradients\n",
    "# 3 - Backwards pass: dLoss/dWeights\n",
    "\n",
    "# Example: linear regression: y_hat = w*x, loss = (y_hat - y)^2 = (w*x - y)^2\n",
    "# Network x,w -> y_hat -> y_hat - y -> s -> s^2 -> loss    (forward pass)\n",
    "# dy_hat/dw <- ds/dy*hat <- dloss/ds <- loss (backward pass)\n",
    "# dLoss/dw <- dLoss/dy_hat <- dLoss/ds\n",
    "\n",
    "# Example: x=1, y=2, w=1\n",
    "\n",
    "# 1 - Forward:  1,1 -> y_hat = 1*1, s = (1-2) = s(-1) = (-1)^2 -> loss = 1\n",
    "\n",
    "# 2 - Calculate local gradients:\n",
    "\n",
    "# dLoss/ds = d(s^2)/ds = 2s\n",
    "# ds/dy_hat = d(y-y_hat)/dy_hat = y\n",
    "# dy_hat/dW = d(w*x)/dw = x\n",
    "\n",
    "# 3 - Backward pass\n",
    "# dLoss/ds = 2s = 2(-1) = -2\n",
    "# ds/dy_hat = y = 1\n",
    "\n",
    "# dLoss/dy_hat = dLoss/ds * ds/dy_hat\n",
    "# dLoss/dy_hat =    -2    *    1           = -2\n",
    "\n",
    "# dLoss/dw = dLoss/dy_hat * dy_hat/dw\n",
    "#          =     -2       *    x\n",
    "#          =     -2       *    1       = -2\n",
    "\n",
    "# dLoss / dw = -2 \n",
    "\n",
    "# PYTORCH VERIFICATION OF CALCULATION ABOVE\n",
    "\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True) # need the gradients\n",
    "\n",
    "# Forward pass\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print(f\"loss={loss}\")\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "print(f\"weight gradient={w.grad}\")    # should be -2 (correct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b4cdbc7c3fa968f8f57a8da50a47c3ee9b8594a4de967130c021b1087ec3a6ed"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
